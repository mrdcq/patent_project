{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b683fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/patent_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908f675",
   "metadata": {},
   "source": [
    "https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html#umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441a084",
   "metadata": {},
   "source": [
    "#### **EMBEDDING MODELS**\n",
    "\n",
    "BERTopic starts with transforming our input documents into numerical representations. Although there are many ways this can be achieved, we typically use sentence-transformers (\"all-MiniLM-L6-v2\") as it is quite capable of capturing the semantic similarity between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f345683",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/mteb/leaderboard\n",
    "FOR BEST EMBEDDING MODELS AS OF NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d955a566",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-base-en-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(embedding_model\u001b[38;5;241m=\u001b[39membedding_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668411ca",
   "metadata": {},
   "source": [
    "##### For custom embeddings:\n",
    "\n",
    "When documents are too specific for a general pre-trained model to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Prepare embeddings\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "# Train our topic model using our pre-trained sentence-transformers embeddings\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e99bc8",
   "metadata": {},
   "source": [
    "#### **DIMENSIONALITY REDUCTION**\n",
    "\n",
    "An important aspect of BERTopic is the dimensionality reduction of the input embeddings. As embeddings are often high in dimensionality, clustering becomes difficult due to the curse of dimensionality.\n",
    "\n",
    "A solution is to reduce the dimensionality of the embeddings to a workable dimensional space (e.g., 5) for clustering algorithms to work with. UMAP is used as a default in BERTopic since it can capture both the local and global high-dimensional space in lower dimensions. However, there are other solutions out there, such as PCA that users might be interested in trying out.\n",
    "\n",
    "NEED TO OPTIMIZE THE PARAMETERS FOR EACH ; could be interesting to look at the proposed solution from : https://medium.com/@boorism/inter-class-clustering-of-text-data-using-dimensionality-reduction-and-bert-390a5f9954b8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ef593",
   "metadata": {},
   "source": [
    "##### UMAP\n",
    "\n",
    "As a default, BERTopic uses UMAP to perform its dimensionality reduction. To use a UMAP model with custom parameters, we simply define it and pass it to BERTopic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "topic_model = BERTopic(umap_model=umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11ef37",
   "metadata": {},
   "source": [
    "##### PCA\n",
    "\n",
    "Although UMAP works quite well in BERTopic and is typically advised, you might want to be using PCA instead. It can be faster to train and perform inference. To use PCA, we can simply import it from sklearn and pass it to the umap_model parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dim_model = PCA(n_components=5)\n",
    "topic_model = BERTopic(umap_model=dim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7bc57",
   "metadata": {},
   "source": [
    "#### **CLUSTERING**\n",
    "\n",
    "After reducing the dimensionality of our input embeddings, we need to cluster them into groups of similar embeddings to extract our topics. This process of clustering is quite important because the more performant our clustering technique the more accurate our topic representations are.\n",
    "\n",
    "In BERTopic, we typically use HDBSCAN as it is quite capable of capturing structures with different densities. However, there is not one perfect clustering model and you might want to be using something entirely different for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de23df4",
   "metadata": {},
   "source": [
    "##### HDBSCAN\n",
    "\n",
    "As a default, BERTopic uses HDBSCAN to perform its clustering. To use a HDBSCAN model with custom parameters, we simply define it and pass it to BERTopic:\n",
    "\n",
    "ONCE AGAIN, NEED TO FINETUNE THE PARAMETERS BEFOREHAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c523ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(hdbscan_model=hdbscan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18909d9a",
   "metadata": {},
   "source": [
    "##### cuML HDBSCAN\n",
    "\n",
    "Although the original HDBSCAN implementation is an amazing technique, it may have difficulty handling large amounts of data. Instead, we can use cuML to speed up HDBSCAN through GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "topic_model = BERTopic(hdbscan_model=hdbscan_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
